{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import reduce, wraps\n",
    "from keras.layers import (Conv2D, Input, ZeroPadding2D, Add, UpSampling2D, MaxPooling2D, Concatenate)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import load_model, Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.vis_utils import plot_model as plot\n",
    "from matplotlib.colors import rgb_to_hsv, hsv_to_rgb\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "from timeit import default_timer as timer\n",
    "import colorsys\n",
    "import configparser\n",
    "import cv2\n",
    "import io\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# compatible with old structure in original yolo implementation\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compose(*funcs):\n",
    "    \"\"\"Compose arbitrarily many functions, evaluated left to right.\n",
    "    Reference: https://mathieularose.com/function-composition-in-python/\n",
    "    \"\"\"\n",
    "    if funcs:\n",
    "        return reduce(lambda f, g: lambda *a, **kw: g(f(*a, **kw)), funcs)\n",
    "    else:\n",
    "        raise ValueError('Composition of empty sequence not supported.')\n",
    "\n",
    "\n",
    "def letterbox_image(image, size):\n",
    "    '''\n",
    "    resize image with unchanged aspect ratio using padding\n",
    "    '''\n",
    "    iw, ih = image.size\n",
    "    w, h = size\n",
    "    scale = min(w / iw, h / ih)\n",
    "    nw = int(iw * scale)\n",
    "    nh = int(ih * scale)\n",
    "\n",
    "    image = image.resize((nw, nh), Image.BICUBIC)\n",
    "    new_image = Image.new('RGB', size, (128, 128, 128))\n",
    "    new_image.paste(image, ((w - nw) // 2, (h - nh) // 2))\n",
    "    return new_image\n",
    "\n",
    "\n",
    "def rand(a=0, b=1):\n",
    "    return np.random.rand() * (b - a) + a\n",
    "\n",
    "\n",
    "def get_random_data(annotation_line, input_shape, random=True, max_boxes=20, jitter=.3, hue=.1, sat=1.5, val=1.5,\n",
    "                    proc_img=True):\n",
    "    '''random preprocessing for real-time data augmentation'''\n",
    "    line = annotation_line.split()\n",
    "    image = Image.open(line[0])\n",
    "    iw, ih = image.size\n",
    "    h, w = input_shape\n",
    "    box = np.array([np.array(list(map(int, box.split(',')))) for box in line[1:]])\n",
    "\n",
    "    if not random:\n",
    "        # resize image\n",
    "        scale = min(w / iw, h / ih)\n",
    "        nw = int(iw * scale)\n",
    "        nh = int(ih * scale)\n",
    "        dx = (w - nw) // 2\n",
    "        dy = (h - nh) // 2\n",
    "        image_data = 0\n",
    "        if proc_img:\n",
    "            image = image.resize((nw, nh), Image.BICUBIC)\n",
    "            new_image = Image.new('RGB', (w, h), (128, 128, 128))\n",
    "            new_image.paste(image, (dx, dy))\n",
    "            image_data = np.array(new_image) / 255.\n",
    "\n",
    "        # correct boxes\n",
    "        box_data = np.zeros((max_boxes, 5))\n",
    "        if len(box) > 0:\n",
    "            np.random.shuffle(box)\n",
    "            if len(box) > max_boxes: box = box[:max_boxes]\n",
    "            box[:, [0, 2]] = box[:, [0, 2]] * scale + dx\n",
    "            box[:, [1, 3]] = box[:, [1, 3]] * scale + dy\n",
    "            box_data[:len(box)] = box\n",
    "\n",
    "        return image_data, box_data\n",
    "\n",
    "    # resize image\n",
    "    new_ar = w / h * rand(1 - jitter, 1 + jitter) / rand(1 - jitter, 1 + jitter)\n",
    "    scale = rand(.25, 2)\n",
    "    if new_ar < 1:\n",
    "        nh = int(scale * h)\n",
    "        nw = int(nh * new_ar)\n",
    "    else:\n",
    "        nw = int(scale * w)\n",
    "        nh = int(nw / new_ar)\n",
    "    image = image.resize((nw, nh), Image.BICUBIC)\n",
    "\n",
    "    # place image\n",
    "    dx = int(rand(0, w - nw))\n",
    "    dy = int(rand(0, h - nh))\n",
    "    new_image = Image.new('RGB', (w, h), (128, 128, 128))\n",
    "    new_image.paste(image, (dx, dy))\n",
    "    image = new_image\n",
    "\n",
    "    # flip image or not\n",
    "    flip = rand() < .5\n",
    "    if flip: image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "    # distort image\n",
    "    hue = rand(-hue, hue)\n",
    "    sat = rand(1, sat) if rand() < .5 else 1 / rand(1, sat)\n",
    "    val = rand(1, val) if rand() < .5 else 1 / rand(1, val)\n",
    "    x = rgb_to_hsv(np.array(image) / 255.)\n",
    "    x[..., 0] += hue\n",
    "    x[..., 0][x[..., 0] > 1] -= 1\n",
    "    x[..., 0][x[..., 0] < 0] += 1\n",
    "    x[..., 1] *= sat\n",
    "    x[..., 2] *= val\n",
    "    x[x > 1] = 1\n",
    "    x[x < 0] = 0\n",
    "    image_data = hsv_to_rgb(x)  # numpy array, 0 to 1\n",
    "\n",
    "    # correct boxes\n",
    "    box_data = np.zeros((max_boxes, 5))\n",
    "    if len(box) > 0:\n",
    "        np.random.shuffle(box)\n",
    "        box[:, [0, 2]] = box[:, [0, 2]] * nw / iw + dx\n",
    "        box[:, [1, 3]] = box[:, [1, 3]] * nh / ih + dy\n",
    "        if flip: box[:, [0, 2]] = w - box[:, [2, 0]]\n",
    "        box[:, 0:2][box[:, 0:2] < 0] = 0\n",
    "        box[:, 2][box[:, 2] > w] = w\n",
    "        box[:, 3][box[:, 3] > h] = h\n",
    "        box_w = box[:, 2] - box[:, 0]\n",
    "        box_h = box[:, 3] - box[:, 1]\n",
    "        box = box[np.logical_and(box_w > 1, box_h > 1)]  # discard invalid box\n",
    "        if len(box) > max_boxes: box = box[:max_boxes]\n",
    "        box_data[:len(box)] = box\n",
    "\n",
    "    return image_data, box_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@wraps(Conv2D)\n",
    "def DarknetConv2D(*args, **kwargs):\n",
    "    \"\"\"Wrapper to set Darknet parameters for Convolution2D.\"\"\"\n",
    "    darknet_conv_kwargs = {'kernel_regularizer': l2(5e-4)}\n",
    "    darknet_conv_kwargs['padding'] = 'valid' if kwargs.get('strides') == (2, 2) else 'same'\n",
    "    darknet_conv_kwargs.update(kwargs)\n",
    "    return Conv2D(*args, **darknet_conv_kwargs)\n",
    "\n",
    "\n",
    "def DarknetConv2D_BN_Leaky(*args, **kwargs):\n",
    "    \"\"\"Darknet Convolution2D followed by BatchNormalization and LeakyReLU.\"\"\"\n",
    "    no_bias_kwargs = {'use_bias': False}\n",
    "    no_bias_kwargs.update(kwargs)\n",
    "    return compose(\n",
    "        DarknetConv2D(*args, **no_bias_kwargs),\n",
    "        BatchNormalization(),\n",
    "        LeakyReLU(alpha=0.1))\n",
    "\n",
    "\n",
    "def resblock_body(x, num_filters, num_blocks):\n",
    "    '''A series of resblocks starting with a downsampling Convolution2D'''\n",
    "    # Darknet uses left and top padding instead of 'same' mode\n",
    "    x = ZeroPadding2D(((1, 0), (1, 0)))(x)\n",
    "    x = DarknetConv2D_BN_Leaky(num_filters, (3, 3), strides=(2, 2))(x)\n",
    "    for i in range(num_blocks):\n",
    "        y = compose(\n",
    "            DarknetConv2D_BN_Leaky(num_filters // 2, (1, 1)),\n",
    "            DarknetConv2D_BN_Leaky(num_filters, (3, 3)))(x)\n",
    "        x = Add()([x, y])\n",
    "    return x\n",
    "\n",
    "\n",
    "def darknet_body(x):\n",
    "    '''Darknent body having 52 Convolution2D layers'''\n",
    "    x = DarknetConv2D_BN_Leaky(32, (3, 3))(x)\n",
    "    x = resblock_body(x, 64, 1)\n",
    "    x = resblock_body(x, 128, 2)\n",
    "    x = resblock_body(x, 256, 8)\n",
    "    x = resblock_body(x, 512, 8)\n",
    "    x = resblock_body(x, 1024, 4)\n",
    "    return x\n",
    "\n",
    "\n",
    "def make_last_layers(x, num_filters, out_filters):\n",
    "    '''6 Conv2D_BN_Leaky layers followed by a Conv2D_linear layer'''\n",
    "    x = compose(\n",
    "        DarknetConv2D_BN_Leaky(num_filters, (1, 1)),\n",
    "        DarknetConv2D_BN_Leaky(num_filters * 2, (3, 3)),\n",
    "        DarknetConv2D_BN_Leaky(num_filters, (1, 1)),\n",
    "        DarknetConv2D_BN_Leaky(num_filters * 2, (3, 3)),\n",
    "        DarknetConv2D_BN_Leaky(num_filters, (1, 1)))(x)\n",
    "    y = compose(\n",
    "        DarknetConv2D_BN_Leaky(num_filters * 2, (3, 3)),\n",
    "        DarknetConv2D(out_filters, (1, 1)))(x)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def yolo_head(feats, anchors, num_classes, input_shape, calc_loss=False):\n",
    "    \"\"\"Convert final layer features to bounding box parameters.\"\"\"\n",
    "    num_anchors = len(anchors)\n",
    "    # Reshape to batch, height, width, num_anchors, box_params.\n",
    "    anchors_tensor = K.reshape(K.constant(anchors), [1, 1, 1, num_anchors, 2])\n",
    "\n",
    "    grid_shape = K.shape(feats)[1:3]  # height, width\n",
    "    grid_y = K.tile(K.reshape(K.arange(0, stop=grid_shape[0]), [-1, 1, 1, 1]),\n",
    "                    [1, grid_shape[1], 1, 1])\n",
    "    grid_x = K.tile(K.reshape(K.arange(0, stop=grid_shape[1]), [1, -1, 1, 1]),\n",
    "                    [grid_shape[0], 1, 1, 1])\n",
    "    grid = K.concatenate([grid_x, grid_y])\n",
    "    grid = K.cast(grid, K.dtype(feats))\n",
    "\n",
    "    feats = K.reshape(\n",
    "        feats, [-1, grid_shape[0], grid_shape[1], num_anchors, num_classes + 5])\n",
    "\n",
    "    # Adjust predictions to each spatial grid point and anchor size.\n",
    "\n",
    "    box_xy = (K.sigmoid(feats[..., :2]) + grid) / K.cast(grid_shape[..., ::-1], K.dtype(feats))\n",
    "    box_wh = K.exp(feats[..., 2:4]) * anchors_tensor / K.cast(input_shape[..., ::-1], K.dtype(feats))\n",
    "\n",
    "    box_confidence = K.sigmoid(feats[..., 4:5])\n",
    "    box_class_probs = K.sigmoid(feats[..., 5:])\n",
    "\n",
    "    if calc_loss == True:\n",
    "        return grid, feats, box_xy, box_wh\n",
    "    return box_xy, box_wh, box_confidence, box_class_probs\n",
    "\n",
    "\n",
    "def yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape):\n",
    "    '''Get corrected boxes'''\n",
    "    box_yx = box_xy[..., ::-1]\n",
    "    box_hw = box_wh[..., ::-1]\n",
    "    input_shape = K.cast(input_shape, K.dtype(box_yx))\n",
    "    image_shape = K.cast(image_shape, K.dtype(box_yx))\n",
    "    new_shape = K.round(image_shape * K.min(input_shape / image_shape))\n",
    "    offset = (input_shape - new_shape) / 2. / input_shape\n",
    "    scale = input_shape / new_shape\n",
    "    box_yx = (box_yx - offset) * scale\n",
    "    box_hw *= scale\n",
    "\n",
    "    box_mins = box_yx - (box_hw / 2.)\n",
    "    box_maxes = box_yx + (box_hw / 2.)\n",
    "    boxes = K.concatenate([\n",
    "        box_mins[..., 0:1],  # y_min\n",
    "        box_mins[..., 1:2],  # x_min\n",
    "        box_maxes[..., 0:1],  # y_max\n",
    "        box_maxes[..., 1:2]  # x_max\n",
    "    ])\n",
    "\n",
    "    # Scale boxes back to original image shape.\n",
    "    boxes *= K.concatenate([image_shape, image_shape])\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def yolo_boxes_and_scores(feats, anchors, num_classes, input_shape, image_shape):\n",
    "    '''Process Conv layer output'''\n",
    "    box_xy, box_wh, box_confidence, box_class_probs = yolo_head(feats,\n",
    "                                                                anchors, num_classes, input_shape)\n",
    "    boxes = yolo_correct_boxes(box_xy, box_wh, input_shape, image_shape)\n",
    "    boxes = K.reshape(boxes, [-1, 4])\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    box_scores = K.reshape(box_scores, [-1, num_classes])\n",
    "    return boxes, box_scores\n",
    "\n",
    "\n",
    "def yolo_eval(yolo_outputs,\n",
    "              anchors,\n",
    "              num_classes,\n",
    "              image_shape,\n",
    "              max_boxes=20,\n",
    "              score_threshold=.6,\n",
    "              iou_threshold=.5):\n",
    "    \"\"\"Evaluate YOLO model on given input and return filtered boxes.\"\"\"\n",
    "    num_layers = len(yolo_outputs)\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]] if num_layers == 3 else [[3, 4, 5], [1, 2, 3]]  # default setting\n",
    "    input_shape = K.shape(yolo_outputs[0])[1:3] * 32\n",
    "    boxes = []\n",
    "    box_scores = []\n",
    "    for l in range(num_layers):\n",
    "        _boxes, _box_scores = yolo_boxes_and_scores(yolo_outputs[l],\n",
    "                                                    anchors[anchor_mask[l]], num_classes, input_shape, image_shape)\n",
    "        boxes.append(_boxes)\n",
    "        box_scores.append(_box_scores)\n",
    "    boxes = K.concatenate(boxes, axis=0)\n",
    "    box_scores = K.concatenate(box_scores, axis=0)\n",
    "\n",
    "    mask = box_scores >= score_threshold\n",
    "    max_boxes_tensor = K.constant(max_boxes, dtype='int32')\n",
    "    boxes_ = []\n",
    "    scores_ = []\n",
    "    classes_ = []\n",
    "    for c in range(num_classes):\n",
    "        # TODO: use keras backend instead of tf.\n",
    "        class_boxes = tf.boolean_mask(boxes, mask[:, c])\n",
    "        class_box_scores = tf.boolean_mask(box_scores[:, c], mask[:, c])\n",
    "        nms_index = tf.image.non_max_suppression(\n",
    "            class_boxes, class_box_scores, max_boxes_tensor, iou_threshold=iou_threshold)\n",
    "        class_boxes = K.gather(class_boxes, nms_index)\n",
    "        class_box_scores = K.gather(class_box_scores, nms_index)\n",
    "        classes = K.ones_like(class_box_scores, 'int32') * c\n",
    "        boxes_.append(class_boxes)\n",
    "        scores_.append(class_box_scores)\n",
    "        classes_.append(classes)\n",
    "    boxes_ = K.concatenate(boxes_, axis=0)\n",
    "    scores_ = K.concatenate(scores_, axis=0)\n",
    "    classes_ = K.concatenate(classes_, axis=0)\n",
    "\n",
    "    return boxes_, scores_, classes_\n",
    "\n",
    "\n",
    "def preprocess_true_boxes(true_boxes, input_shape, anchors, num_classes):\n",
    "    '''Preprocess true boxes to training input format\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes: array, shape=(m, T, 5)\n",
    "        Absolute x_min, y_min, x_max, y_max, class_id relative to input_shape.\n",
    "    input_shape: array-like, hw, multiples of 32\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "    Returns\n",
    "    -------\n",
    "    y_true: list of array, shape like yolo_outputs, xywh are reletive value\n",
    "    '''\n",
    "    assert (true_boxes[..., 4] < num_classes).all(), 'class id must be less than num_classes'\n",
    "    num_layers = len(anchors) // 3  # default setting\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]] if num_layers == 3 else [[3, 4, 5], [1, 2, 3]]\n",
    "\n",
    "    true_boxes = np.array(true_boxes, dtype='float32')\n",
    "    input_shape = np.array(input_shape, dtype='int32')\n",
    "    boxes_xy = (true_boxes[..., 0:2] + true_boxes[..., 2:4]) // 2\n",
    "    boxes_wh = true_boxes[..., 2:4] - true_boxes[..., 0:2]\n",
    "    true_boxes[..., 0:2] = boxes_xy / input_shape[::-1]\n",
    "    true_boxes[..., 2:4] = boxes_wh / input_shape[::-1]\n",
    "\n",
    "    m = true_boxes.shape[0]\n",
    "    grid_shapes = [input_shape // {0: 32, 1: 16, 2: 8}[l] for l in range(num_layers)]\n",
    "    y_true = [np.zeros((m, grid_shapes[l][0], grid_shapes[l][1], len(anchor_mask[l]), 5 + num_classes),\n",
    "                       dtype='float32') for l in range(num_layers)]\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    anchors = np.expand_dims(anchors, 0)\n",
    "    anchor_maxes = anchors / 2.\n",
    "    anchor_mins = -anchor_maxes\n",
    "    valid_mask = boxes_wh[..., 0] > 0\n",
    "\n",
    "    for b in range(m):\n",
    "        # Discard zero rows.\n",
    "        wh = boxes_wh[b, valid_mask[b]]\n",
    "        if len(wh) == 0: continue\n",
    "        # Expand dim to apply broadcasting.\n",
    "        wh = np.expand_dims(wh, -2)\n",
    "        box_maxes = wh / 2.\n",
    "        box_mins = -box_maxes\n",
    "\n",
    "        intersect_mins = np.maximum(box_mins, anchor_mins)\n",
    "        intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n",
    "        intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        box_area = wh[..., 0] * wh[..., 1]\n",
    "        anchor_area = anchors[..., 0] * anchors[..., 1]\n",
    "        iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "\n",
    "        # Find best anchor for each true box\n",
    "        best_anchor = np.argmax(iou, axis=-1)\n",
    "\n",
    "        for t, n in enumerate(best_anchor):\n",
    "            for l in range(num_layers):\n",
    "                if n in anchor_mask[l]:\n",
    "                    i = np.floor(true_boxes[b, t, 0] * grid_shapes[l][1]).astype('int32')\n",
    "                    j = np.floor(true_boxes[b, t, 1] * grid_shapes[l][0]).astype('int32')\n",
    "                    k = anchor_mask[l].index(n)\n",
    "                    c = true_boxes[b, t, 4].astype('int32')\n",
    "                    y_true[l][b, j, i, k, 0:4] = true_boxes[b, t, 0:4]\n",
    "                    y_true[l][b, j, i, k, 4] = 1\n",
    "                    y_true[l][b, j, i, k, 5 + c] = 1\n",
    "\n",
    "    return y_true\n",
    "\n",
    "\n",
    "def box_iou(b1, b2):\n",
    "    '''Return iou tensor\n",
    "    Parameters\n",
    "    ----------\n",
    "    b1: tensor, shape=(i1,...,iN, 4), xywh\n",
    "    b2: tensor, shape=(j, 4), xywh\n",
    "    Returns\n",
    "    -------\n",
    "    iou: tensor, shape=(i1,...,iN, j)\n",
    "    '''\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b1 = K.expand_dims(b1, -2)\n",
    "    b1_xy = b1[..., :2]\n",
    "    b1_wh = b1[..., 2:4]\n",
    "    b1_wh_half = b1_wh / 2.\n",
    "    b1_mins = b1_xy - b1_wh_half\n",
    "    b1_maxes = b1_xy + b1_wh_half\n",
    "\n",
    "    # Expand dim to apply broadcasting.\n",
    "    b2 = K.expand_dims(b2, 0)\n",
    "    b2_xy = b2[..., :2]\n",
    "    b2_wh = b2[..., 2:4]\n",
    "    b2_wh_half = b2_wh / 2.\n",
    "    b2_mins = b2_xy - b2_wh_half\n",
    "    b2_maxes = b2_xy + b2_wh_half\n",
    "\n",
    "    intersect_mins = K.maximum(b1_mins, b2_mins)\n",
    "    intersect_maxes = K.minimum(b1_maxes, b2_maxes)\n",
    "    intersect_wh = K.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_area = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    b1_area = b1_wh[..., 0] * b1_wh[..., 1]\n",
    "    b2_area = b2_wh[..., 0] * b2_wh[..., 1]\n",
    "    iou = intersect_area / (b1_area + b2_area - intersect_area)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def yolo_loss(args, anchors, num_classes, ignore_thresh=.5, print_loss=False):\n",
    "    '''Return yolo_loss tensor\n",
    "    Parameters\n",
    "    ----------\n",
    "    yolo_outputs: list of tensor, the output of yolo_body or tiny_yolo_body\n",
    "    y_true: list of array, the output of preprocess_true_boxes\n",
    "    anchors: array, shape=(N, 2), wh\n",
    "    num_classes: integer\n",
    "    ignore_thresh: float, the iou threshold whether to ignore object confidence loss\n",
    "    Returns\n",
    "    -------\n",
    "    loss: tensor, shape=(1,)\n",
    "    '''\n",
    "    num_layers = len(anchors) // 3  # default setting\n",
    "    yolo_outputs = args[:num_layers]\n",
    "    y_true = args[num_layers:]\n",
    "    anchor_mask = [[6, 7, 8], [3, 4, 5], [0, 1, 2]] if num_layers == 3 else [[3, 4, 5], [1, 2, 3]]\n",
    "    input_shape = K.cast(K.shape(yolo_outputs[0])[1:3] * 32, K.dtype(y_true[0]))\n",
    "    grid_shapes = [K.cast(K.shape(yolo_outputs[l])[1:3], K.dtype(y_true[0])) for l in range(num_layers)]\n",
    "    loss = 0\n",
    "    m = K.shape(yolo_outputs[0])[0]  # batch size, tensor\n",
    "    mf = K.cast(m, K.dtype(yolo_outputs[0]))\n",
    "\n",
    "    for l in range(num_layers):\n",
    "        object_mask = y_true[l][..., 4:5]\n",
    "        true_class_probs = y_true[l][..., 5:]\n",
    "\n",
    "        grid, raw_pred, pred_xy, pred_wh = yolo_head(yolo_outputs[l],\n",
    "                                                     anchors[anchor_mask[l]], num_classes, input_shape, calc_loss=True)\n",
    "        pred_box = K.concatenate([pred_xy, pred_wh])\n",
    "\n",
    "        # Darknet raw box to calculate loss.\n",
    "        raw_true_xy = y_true[l][..., :2] * grid_shapes[l][::-1] - grid\n",
    "        raw_true_wh = K.log(y_true[l][..., 2:4] / anchors[anchor_mask[l]] * input_shape[::-1])\n",
    "        raw_true_wh = K.switch(object_mask, raw_true_wh, K.zeros_like(raw_true_wh))  # avoid log(0)=-inf\n",
    "        box_loss_scale = 2 - y_true[l][..., 2:3] * y_true[l][..., 3:4]\n",
    "\n",
    "        # Find ignore mask, iterate over each of batch.\n",
    "        ignore_mask = tf.TensorArray(K.dtype(y_true[0]), size=1, dynamic_size=True)\n",
    "        object_mask_bool = K.cast(object_mask, 'bool')\n",
    "\n",
    "        def loop_body(b, ignore_mask):\n",
    "            true_box = tf.boolean_mask(y_true[l][b, ..., 0:4], object_mask_bool[b, ..., 0])\n",
    "            iou = box_iou(pred_box[b], true_box)\n",
    "            best_iou = K.max(iou, axis=-1)\n",
    "            ignore_mask = ignore_mask.write(b, K.cast(best_iou < ignore_thresh, K.dtype(true_box)))\n",
    "            return b + 1, ignore_mask\n",
    "\n",
    "        _, ignore_mask = K.control_flow_ops.while_loop(lambda b, *args: b < m, loop_body, [0, ignore_mask])\n",
    "        ignore_mask = ignore_mask.stack()\n",
    "        ignore_mask = K.expand_dims(ignore_mask, -1)\n",
    "\n",
    "        # K.binary_crossentropy is helpful to avoid exp overflow.\n",
    "        xy_loss = object_mask * box_loss_scale * K.binary_crossentropy(raw_true_xy, raw_pred[..., 0:2],\n",
    "                                                                       from_logits=True)\n",
    "        wh_loss = object_mask * box_loss_scale * 0.5 * K.square(raw_true_wh - raw_pred[..., 2:4])\n",
    "        confidence_loss = (object_mask * K.binary_crossentropy(object_mask, raw_pred[..., 4:5], from_logits=True) +\n",
    "                           (1 - object_mask) * K.binary_crossentropy(object_mask, raw_pred[..., 4:5],\n",
    "                                                                     from_logits=True) * ignore_mask)\n",
    "\n",
    "        class_loss = object_mask * K.binary_crossentropy(true_class_probs, raw_pred[..., 5:], from_logits=True)\n",
    "\n",
    "        xy_loss = K.sum(xy_loss) / mf\n",
    "        wh_loss = K.sum(wh_loss) / mf\n",
    "        confidence_loss = K.sum(confidence_loss) / mf\n",
    "        class_loss = K.sum(class_loss) / mf\n",
    "        loss += xy_loss + wh_loss + confidence_loss + class_loss\n",
    "        if print_loss:\n",
    "            loss = tf.Print(loss, [loss, xy_loss, wh_loss, confidence_loss, class_loss, K.sum(ignore_mask)],\n",
    "                            message='loss: ')\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def unique_config_sections(config_file):\n",
    "    section_counters = defaultdict(int)\n",
    "    output_stream = io.StringIO()\n",
    "    with open(config_file) as fin:\n",
    "        for line in fin:\n",
    "            if line.startswith('['):\n",
    "                section = line.strip().strip('[]')\n",
    "                _section = section + '_' + str(section_counters[section])\n",
    "                section_counters[section] += 1\n",
    "                line = line.replace(section, _section)\n",
    "            output_stream.write(line)\n",
    "    output_stream.seek(0)\n",
    "    return output_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def _main(args):\n",
    "    config_path = args[0]\n",
    "    weights_path = args[1]\n",
    "    output_path = args[2]\n",
    "\n",
    "    output_root = os.path.splitext(output_path)[0]\n",
    "\n",
    "    # Load weights and config.\n",
    "    print('Loading weights.')\n",
    "    weights_file = open(weights_path, 'rb')\n",
    "    major, minor, revision = np.ndarray(\n",
    "        shape=(3,), dtype='int32', buffer=weights_file.read(12))\n",
    "    if (major * 10 + minor) >= 2 and major < 1000 and minor < 1000:\n",
    "        seen = np.ndarray(shape=(1,), dtype='int64', buffer=weights_file.read(8))\n",
    "    else:\n",
    "        seen = np.ndarray(shape=(1,), dtype='int32', buffer=weights_file.read(4))\n",
    "    print('Weights Header: ', major, minor, revision, seen)\n",
    "\n",
    "    print('Parsing Darknet config.')\n",
    "    unique_config_file = unique_config_sections(config_path)\n",
    "    cfg_parser = configparser.ConfigParser()\n",
    "    cfg_parser.read_file(unique_config_file)\n",
    "\n",
    "    print('Creating Keras model.')\n",
    "    input_layer = Input(shape=(None, None, 3))\n",
    "    prev_layer = input_layer\n",
    "    all_layers = []\n",
    "\n",
    "    weight_decay = float(cfg_parser['net_0']['decay']\n",
    "                         ) if 'net_0' in cfg_parser.sections() else 5e-4\n",
    "    count = 0\n",
    "    out_index = []\n",
    "    for section in cfg_parser.sections():\n",
    "        print('Parsing section {}'.format(section))\n",
    "        if section.startswith('convolutional'):\n",
    "            filters = int(cfg_parser[section]['filters'])\n",
    "            size = int(cfg_parser[section]['size'])\n",
    "            stride = int(cfg_parser[section]['stride'])\n",
    "            pad = int(cfg_parser[section]['pad'])\n",
    "            activation = cfg_parser[section]['activation']\n",
    "            batch_normalize = 'batch_normalize' in cfg_parser[section]\n",
    "\n",
    "            padding = 'same' if pad == 1 and stride == 1 else 'valid'\n",
    "\n",
    "            # Setting weights.\n",
    "            # Darknet serializes convolutional weights as:\n",
    "            # [bias/beta, [gamma, mean, variance], conv_weights]\n",
    "            prev_layer_shape = K.int_shape(prev_layer)\n",
    "\n",
    "            weights_shape = (size, size, prev_layer_shape[-1], filters)\n",
    "            darknet_w_shape = (filters, weights_shape[2], size, size)\n",
    "            weights_size = np.product(weights_shape)\n",
    "\n",
    "            print('conv2d', 'bn'\n",
    "            if batch_normalize else '  ', activation, weights_shape)\n",
    "\n",
    "            conv_bias = np.ndarray(\n",
    "                shape=(filters,),\n",
    "                dtype='float32',\n",
    "                buffer=weights_file.read(filters * 4))\n",
    "            count += filters\n",
    "\n",
    "            if batch_normalize:\n",
    "                bn_weights = np.ndarray(\n",
    "                    shape=(3, filters),\n",
    "                    dtype='float32',\n",
    "                    buffer=weights_file.read(filters * 12))\n",
    "                count += 3 * filters\n",
    "\n",
    "                bn_weight_list = [\n",
    "                    bn_weights[0],  # scale gamma\n",
    "                    conv_bias,  # shift beta\n",
    "                    bn_weights[1],  # running mean\n",
    "                    bn_weights[2]  # running var\n",
    "                ]\n",
    "\n",
    "            conv_weights = np.ndarray(\n",
    "                shape=darknet_w_shape,\n",
    "                dtype='float32',\n",
    "                buffer=weights_file.read(weights_size * 4))\n",
    "            count += weights_size\n",
    "\n",
    "            # DarkNet conv_weights are serialized Caffe-style:\n",
    "            # (out_dim, in_dim, height, width)\n",
    "            # We would like to set these to Tensorflow order:\n",
    "            # (height, width, in_dim, out_dim)\n",
    "            conv_weights = np.transpose(conv_weights, [2, 3, 1, 0])\n",
    "            conv_weights = [conv_weights] if batch_normalize else [\n",
    "                conv_weights, conv_bias\n",
    "            ]\n",
    "\n",
    "            # Handle activation.\n",
    "            act_fn = None\n",
    "            if activation == 'leaky':\n",
    "                pass  # Add advanced activation later.\n",
    "            elif activation != 'linear':\n",
    "                raise ValueError(\n",
    "                    'Unknown activation function `{}` in section {}'.format(\n",
    "                        activation, section))\n",
    "\n",
    "            # Create Conv2D layer\n",
    "            if stride > 1:\n",
    "                # Darknet uses left and top padding instead of 'same' mode\n",
    "                prev_layer = ZeroPadding2D(((1, 0), (1, 0)))(prev_layer)\n",
    "            conv_layer = (Conv2D(\n",
    "                filters, (size, size),\n",
    "                strides=(stride, stride),\n",
    "                kernel_regularizer=l2(weight_decay),\n",
    "                use_bias=not batch_normalize,\n",
    "                weights=conv_weights,\n",
    "                activation=act_fn,\n",
    "                padding=padding))(prev_layer)\n",
    "\n",
    "            if batch_normalize:\n",
    "                conv_layer = (BatchNormalization(\n",
    "                    weights=bn_weight_list))(conv_layer)\n",
    "            prev_layer = conv_layer\n",
    "\n",
    "            if activation == 'linear':\n",
    "                all_layers.append(prev_layer)\n",
    "            elif activation == 'leaky':\n",
    "                act_layer = LeakyReLU(alpha=0.1)(prev_layer)\n",
    "                prev_layer = act_layer\n",
    "                all_layers.append(act_layer)\n",
    "\n",
    "        elif section.startswith('route'):\n",
    "            ids = [int(i) for i in cfg_parser[section]['layers'].split(',')]\n",
    "            layers = [all_layers[i] for i in ids]\n",
    "            if len(layers) > 1:\n",
    "                print('Concatenating route layers:', layers)\n",
    "                concatenate_layer = Concatenate()(layers)\n",
    "                all_layers.append(concatenate_layer)\n",
    "                prev_layer = concatenate_layer\n",
    "            else:\n",
    "                skip_layer = layers[0]  # only one layer to route\n",
    "                all_layers.append(skip_layer)\n",
    "                prev_layer = skip_layer\n",
    "\n",
    "        elif section.startswith('maxpool'):\n",
    "            size = int(cfg_parser[section]['size'])\n",
    "            stride = int(cfg_parser[section]['stride'])\n",
    "            all_layers.append(\n",
    "                MaxPooling2D(\n",
    "                    pool_size=(size, size),\n",
    "                    strides=(stride, stride),\n",
    "                    padding='same')(prev_layer))\n",
    "            prev_layer = all_layers[-1]\n",
    "\n",
    "        elif section.startswith('shortcut'):\n",
    "            index = int(cfg_parser[section]['from'])\n",
    "            activation = cfg_parser[section]['activation']\n",
    "            assert activation == 'linear', 'Only linear activation supported.'\n",
    "            all_layers.append(Add()([all_layers[index], prev_layer]))\n",
    "            prev_layer = all_layers[-1]\n",
    "\n",
    "        elif section.startswith('upsample'):\n",
    "            stride = int(cfg_parser[section]['stride'])\n",
    "            assert stride == 2, 'Only stride=2 supported.'\n",
    "            all_layers.append(UpSampling2D(stride)(prev_layer))\n",
    "            prev_layer = all_layers[-1]\n",
    "\n",
    "        elif section.startswith('yolo'):\n",
    "            out_index.append(len(all_layers) - 1)\n",
    "            all_layers.append(None)\n",
    "            prev_layer = all_layers[-1]\n",
    "\n",
    "        elif section.startswith('net'):\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Unsupported section header type: {}'.format(section))\n",
    "\n",
    "    # Create and save model.\n",
    "    if len(out_index) == 0: out_index.append(len(all_layers) - 1)\n",
    "    model = Model(inputs=input_layer, outputs=[all_layers[i] for i in out_index])\n",
    "    print(model.summary())\n",
    "    model.save('{}'.format(output_path))\n",
    "    print('Saved Keras model to {}'.format(output_path))\n",
    "\n",
    "    # Check to see if all weights have been read.\n",
    "    remaining_weights = len(weights_file.read()) / 4\n",
    "    weights_file.close()\n",
    "    print('Read {} of {} from Darknet weights.'.format(count, count +\n",
    "                                                       remaining_weights))\n",
    "    if remaining_weights > 0:\n",
    "        print('Warning: {} unused weights'.format(remaining_weights))\n",
    "\n",
    "    plot(model, to_file='{}.png'.format(output_root), show_shapes=True)\n",
    "    print('Saved model plot to {}.png'.format(output_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "args = ['wider.cfg', 'wider.weights', 'wider.h5']  #face only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights.\n",
      "Weights Header:  1936876918 544108393 1886680168 [791624307]\n",
      "Parsing Darknet config.\n",
      "Creating Keras model.\n",
      "Parsing section net_0\n",
      "Parsing section convolutional_0\n",
      "conv2d bn leaky (3, 3, 3, 32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "buffer is too small for requested array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8b35bdbcbe41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-41884fd0c703>\u001b[0m in \u001b[0;36m_main\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     55\u001b[0m             if batch_normalize else '  ', activation, weights_shape)\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             conv_bias = np.ndarray(\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: buffer is too small for requested array"
     ]
    }
   ],
   "source": [
    "_main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "target_vid = 'CHUNG_HA Bicycle.mp4'  # fill this with name of vid\n",
    "predicted_vid = 'CHUNG_HA Bicycle predicted.mp4'  # fill this with destination of ouput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class YOLO(object):\n",
    "    _defaults = {\n",
    "        \"model_path\": 'yolo.h5',\n",
    "        \"anchors_path\": 'yolo_anchors.txt',\n",
    "        \"classes_path\": 'coco_classes.txt',\n",
    "        \"score\": 0.3,\n",
    "        \"iou\": 0.45,\n",
    "        \"model_image_size\": (416, 416),\n",
    "        \"gpu_num\": 0,\n",
    "    }\n",
    "\n",
    "    @classmethod\n",
    "    def get_defaults(cls, n):\n",
    "        if n in cls._defaults:\n",
    "            return cls._defaults[n]\n",
    "        else:\n",
    "            return \"Unrecognized attribute name '\" + n + \"'\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(self._defaults)  # set up default values\n",
    "        self.__dict__.update(kwargs)  # and update with user overrides\n",
    "        self.class_names = self._get_class()\n",
    "        self.anchors = self._get_anchors()\n",
    "        self.sess = tf.compat.v1.keras.backend.get_session()\n",
    "        self.boxes, self.scores, self.classes = self.generate()\n",
    "\n",
    "    def _get_class(self):\n",
    "        classes_path = os.path.expanduser(self.classes_path)\n",
    "        with open(classes_path) as f:\n",
    "            class_names = f.readlines()\n",
    "        class_names = [c.strip() for c in class_names]\n",
    "        return class_names\n",
    "\n",
    "    def _get_anchors(self):\n",
    "        anchors_path = os.path.expanduser(self.anchors_path)\n",
    "        with open(anchors_path) as f:\n",
    "            anchors = f.readline()\n",
    "        anchors = [float(x) for x in anchors.split(',')]\n",
    "        return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "    def generate(self):\n",
    "        model_path = os.path.expanduser(self.model_path)\n",
    "        # assert model_path.endswith('.h5'), 'Keras model or weights must be a .h5 file.'\n",
    "\n",
    "        # Load model, or construct model and load weights.\n",
    "        num_anchors = len(self.anchors)\n",
    "        num_classes = len(self.class_names)\n",
    "        print(model_path.encode('utf-8'))\n",
    "        self.yolo_model = load_model(model_path, compile=False)\n",
    "\n",
    "        print('{} model, anchors, and classes loaded.'.format(model_path))\n",
    "\n",
    "        # Generate colors for drawing bounding boxes.\n",
    "        hsv_tuples = [(x / len(self.class_names), 1., 1.)\n",
    "                      for x in range(len(self.class_names))]\n",
    "        self.colors = list(map(lambda x: colorsys.hsv_to_rgb(*x), hsv_tuples))\n",
    "        self.colors = list(\n",
    "            map(lambda x: (int(x[0] * 255), int(x[1] * 255), int(x[2] * 255)),\n",
    "                self.colors))\n",
    "        np.random.seed(10101)  # Fixed seed for consistent colors across runs.\n",
    "        np.random.shuffle(self.colors)  # Shuffle colors to decorrelate adjacent classes.\n",
    "        np.random.seed(None)  # Reset seed to default.\n",
    "\n",
    "        # Generate output tensor targets for filtered bounding boxes.\n",
    "        self.input_image_shape = K.placeholder(shape=(2,))\n",
    "        boxes, scores, classes = yolo_eval(\n",
    "            self.yolo_model.output, self.anchors,\n",
    "            len(self.class_names), self.input_image_shape,\n",
    "            score_threshold=self.score, iou_threshold=self.iou)\n",
    "        return boxes, scores, classes\n",
    "\n",
    "    def detect_image(self, image):\n",
    "        start = timer()\n",
    "\n",
    "        if self.model_image_size != (None, None):\n",
    "            assert self.model_image_size[0] % 32 == 0, 'Multiples of 32 required'\n",
    "            assert self.model_image_size[1] % 32 == 0, 'Multiples of 32 required'\n",
    "            boxed_image = letterbox_image(image, tuple(reversed(self.model_image_size)))\n",
    "        else:\n",
    "            new_image_size = (image.width - (image.width % 32),\n",
    "                              image.height - (image.height % 32))\n",
    "            boxed_image = letterbox_image(image, new_image_size)\n",
    "        image_data = np.array(boxed_image, dtype='float32')\n",
    "\n",
    "        print(image_data.shape)\n",
    "        image_data /= 255.\n",
    "        image_data = np.expand_dims(image_data, 0)  # Add batch dimension.\n",
    "\n",
    "        out_boxes, out_scores, out_classes = self.sess.run(\n",
    "            [self.boxes, self.scores, self.classes],\n",
    "            feed_dict={\n",
    "                self.yolo_model.input: image_data,\n",
    "                self.input_image_shape: [image.size[1], image.size[0]],\n",
    "                # K.learning_phase(): 0\n",
    "            })\n",
    "\n",
    "        print('Found {} boxes for {}'.format(len(out_boxes), 'img'))\n",
    "\n",
    "        font = ImageFont.truetype(font='./FiraMono-Medium.otf',\n",
    "                                  size=np.floor(3e-2 * image.size[1] + 0.5).astype('int32'))\n",
    "        # font  = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        thickness = (image.size[0] + image.size[1]) // 300\n",
    "\n",
    "        for i, c in reversed(list(enumerate(out_classes))):\n",
    "            predicted_class = self.class_names[c]\n",
    "            box = out_boxes[i]\n",
    "            score = out_scores[i]\n",
    "\n",
    "            label = '{} {:.2f}'.format(predicted_class, score)\n",
    "            draw = ImageDraw.Draw(image)\n",
    "            label_size = draw.textsize(label, font)\n",
    "            # label_size = draw.textsize(label)\n",
    "\n",
    "            top, left, bottom, right = box\n",
    "            top = max(0, np.floor(top + 0.5).astype('int32'))\n",
    "            left = max(0, np.floor(left + 0.5).astype('int32'))\n",
    "            bottom = min(image.size[1], np.floor(bottom + 0.5).astype('int32'))\n",
    "            right = min(image.size[0], np.floor(right + 0.5).astype('int32'))\n",
    "            print(label, (left, top), (right, bottom))\n",
    "\n",
    "            if top - label_size[1] >= 0:\n",
    "                text_origin = np.array([left, top - label_size[1]])\n",
    "            else:\n",
    "                text_origin = np.array([left, top + 1])\n",
    "\n",
    "            # My kingdom for a good redistributable image drawing library.\n",
    "            for i in range(thickness):\n",
    "                draw.rectangle(\n",
    "                    [left + i, top + i, right - i, bottom - i],\n",
    "                    outline=self.colors[c])\n",
    "            draw.rectangle(\n",
    "                [tuple(text_origin), tuple(text_origin + label_size)],\n",
    "                fill=self.colors[c])\n",
    "            draw.text(text_origin, label, fill=(0, 0, 0), font=font)\n",
    "\n",
    "            del draw\n",
    "\n",
    "        end = timer()\n",
    "        print(end - start)\n",
    "        return image\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def detect_video(yolo, video_path, output_path=\"\"):\n",
    "    vid = cv2.VideoCapture(video_path)\n",
    "    if not vid.isOpened():\n",
    "        raise IOError(\"Couldn't open webcam or video\")\n",
    "    video_FourCC = int(vid.get(cv2.CAP_PROP_FOURCC))\n",
    "    video_fps = vid.get(cv2.CAP_PROP_FPS)\n",
    "    video_size = (int(vid.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "                  int(vid.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "    isOutput = True if output_path != \"\" else False\n",
    "    if isOutput:\n",
    "        print(\"!!! TYPE:\", type(output_path), type(video_FourCC), type(video_fps), type(video_size))\n",
    "        out = cv2.VideoWriter(output_path, video_FourCC, video_fps, video_size)\n",
    "    accum_time = 0\n",
    "    curr_fps = 0\n",
    "    fps = \"FPS: ??\"\n",
    "    prev_time = timer()\n",
    "    while True:\n",
    "        return_value, frame = vid.read()  # blocking operation. may be delegated to another thread\n",
    "\n",
    "        if return_value is False:\n",
    "            break\n",
    "\n",
    "        image = Image.fromarray(frame)\n",
    "        image = yolo.detect_image(image)\n",
    "        result = np.asarray(image)\n",
    "        curr_time = timer()\n",
    "        exec_time = curr_time - prev_time\n",
    "        prev_time = curr_time\n",
    "        accum_time = accum_time + exec_time\n",
    "        curr_fps = curr_fps + 1\n",
    "        if accum_time > 1:\n",
    "            accum_time = accum_time - 1\n",
    "            fps = \"FPS: \" + str(curr_fps)\n",
    "            curr_fps = 0\n",
    "        cv2.putText(result, text=fps, org=(3, 15), fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    fontScale=0.50, color=(255, 0, 0), thickness=2)\n",
    "        cv2.namedWindow(\"result\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"result\", result)\n",
    "        if isOutput:\n",
    "            out.write(result)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    yolo.close_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "face_config = {\"model_path\": 'wider.h5',\n",
    "               \"anchors_path\": 'wider_anchors.txt',\n",
    "               \"classes_path\": 'wider_classes.txt',\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# use genral config for original model, face_config for face only\n",
    "# detect_video(YOLO(**face_config), target_vid, predicted_vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'wider.h5'\n",
      "wider.h5 model, anchors, and classes loaded.\n"
     ]
    }
   ],
   "source": [
    "yolo_loaded = YOLO(**face_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_path = 'samples/lhvn_free.jpg'\n",
    "\n",
    "img = Image.open(image_path)\n",
    "img.show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(416, 416, 3)\n",
      "Found 12 boxes for img\n",
      "face 0.54 (499, 205) (532, 243)\n",
      "face 0.88 (101, 237) (126, 263)\n",
      "face 0.91 (895, 182) (927, 219)\n",
      "face 0.91 (396, 45) (424, 80)\n",
      "face 0.94 (932, 79) (953, 107)\n",
      "face 0.97 (764, 81) (784, 108)\n",
      "face 0.97 (431, 209) (469, 259)\n",
      "face 0.97 (490, 264) (526, 301)\n",
      "face 0.98 (582, 242) (632, 297)\n",
      "face 0.99 (874, 208) (916, 258)\n",
      "face 1.00 (210, 247) (256, 302)\n",
      "face 1.00 (481, 326) (519, 371)\n",
      "2.37708815500082\n"
     ]
    }
   ],
   "source": [
    "pred_img = yolo_loaded.detect_image(img)\n",
    "pred_img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples/lhvn_free_predicted.jpg\n"
     ]
    }
   ],
   "source": [
    "head_tail = os.path.split(image_path)\n",
    "try_split = head_tail[1].split('.')\n",
    "new_file_name = '.'.join(try_split[:-1]) + '_predicted.' + try_split[-1]\n",
    "\n",
    "dest_img_path = os.path.join(head_tail[0], new_file_name)\n",
    "print(dest_img_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pred_img.save(dest_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'wider.h5'\n",
      "wider.h5 model, anchors, and classes loaded.\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 71) (602, 133)\n",
      "2.251968182999917\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 71) (602, 133)\n",
      "0.3153975660006836\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 72) (602, 133)\n",
      "0.29832997099947534\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 73) (602, 133)\n",
      "0.3151287479995517\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 75) (605, 135)\n",
      "0.31473004200051946\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 76) (604, 136)\n",
      "0.3209874040003342\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 0.99 (556, 74) (602, 136)\n",
      "0.3231929230005335\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 0.99 (557, 81) (602, 142)\n",
      "0.3151882170004683\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 82) (605, 143)\n",
      "0.30869129200073075\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 82) (605, 142)\n",
      "0.33567891900020186\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 81) (605, 142)\n",
      "0.3241738010001427\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 75) (605, 136)\n",
      "0.3130539249996218\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 74) (605, 137)\n",
      "0.3246324360006838\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 73) (605, 137)\n",
      "0.31168733200138377\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (553, 73) (605, 136)\n",
      "0.30888973100081785\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (553, 72) (605, 135)\n",
      "0.32744452899896714\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 71) (605, 135)\n",
      "0.31990924199999426\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 70) (605, 133)\n",
      "0.29459520499949576\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 69) (605, 132)\n",
      "0.30902175000119314\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 67) (605, 131)\n",
      "0.29993061300046975\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 67) (604, 130)\n",
      "0.3181548809989181\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 66) (605, 129)\n",
      "0.30558389199904923\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (550, 65) (604, 128)\n",
      "0.3382560249992821\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (550, 64) (605, 128)\n",
      "0.33129628900132957\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (550, 63) (605, 128)\n",
      "0.33143445099995006\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (550, 64) (606, 128)\n",
      "0.316546787000334\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 64) (606, 129)\n",
      "0.31293713100058085\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 64) (607, 131)\n",
      "0.3178777609991812\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 64) (608, 132)\n",
      "0.3104955470007553\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (553, 65) (608, 132)\n",
      "0.3216307160000724\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (553, 67) (608, 133)\n",
      "0.3158649939996394\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (553, 67) (609, 134)\n",
      "0.32979354099916236\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 68) (609, 137)\n",
      "0.30180952199953026\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (555, 69) (610, 139)\n",
      "0.3053255499999068\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 69) (610, 139)\n",
      "0.3167720729998109\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 68) (610, 140)\n",
      "0.2915695149986277\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 69) (610, 140)\n",
      "0.31059705000006943\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 68) (611, 141)\n",
      "0.3194402810004249\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 68) (611, 140)\n",
      "0.30506830500053184\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 68) (611, 139)\n",
      "0.2986526069998945\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 68) (611, 138)\n",
      "0.3181118390002666\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 67) (611, 138)\n",
      "0.30336261599950376\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 67) (611, 140)\n",
      "0.3044631829998252\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 67) (611, 140)\n",
      "0.3209010880000278\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (556, 66) (611, 140)\n",
      "0.32602024399966467\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (555, 66) (612, 139)\n",
      "0.31853832400156534\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 65) (611, 138)\n",
      "0.33198487399931764\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 64) (611, 137)\n",
      "0.31391638399873045\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (553, 58) (609, 143)\n",
      "0.3120028060002369\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (553, 57) (609, 143)\n",
      "0.3128666669999802\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 56) (609, 142)\n",
      "0.31127454899979057\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 56) (609, 142)\n",
      "0.3210193569993862\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 55) (609, 140)\n",
      "0.3037039200007712\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 55) (609, 139)\n",
      "0.327611259999685\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 55) (610, 139)\n",
      "0.29901660900031857\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 54) (610, 138)\n",
      "0.3236351919986191\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 53) (611, 138)\n",
      "0.31561212499946123\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 53) (612, 138)\n",
      "0.3111485539993737\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (553, 53) (613, 138)\n",
      "0.32598510700154293\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 53) (614, 138)\n",
      "0.31201048500042816\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 53) (615, 139)\n",
      "0.3140175030002865\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (555, 53) (614, 139)\n",
      "0.30904203000136476\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (555, 54) (614, 142)\n",
      "0.322678149001149\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (555, 55) (614, 143)\n",
      "0.3264463840005192\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 55) (613, 143)\n",
      "0.29822641399914573\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (554, 56) (614, 143)\n",
      "0.3514937590007321\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 57) (614, 144)\n",
      "0.3169304340008239\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (552, 57) (613, 145)\n",
      "0.31620316699991236\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 57) (613, 145)\n",
      "0.3581112199990457\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (551, 57) (613, 145)\n",
      "0.3267421149994334\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (550, 56) (613, 144)\n",
      "0.4582841199990071\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (550, 55) (612, 143)\n",
      "0.6347204009998677\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (549, 54) (612, 143)\n",
      "0.3692365780007094\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (549, 54) (612, 145)\n",
      "0.3890480509999179\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (549, 54) (612, 143)\n",
      "0.37253311300082714\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (549, 54) (612, 143)\n",
      "0.3162352009985625\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (549, 53) (613, 143)\n",
      "0.31703472399931343\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (549, 53) (612, 144)\n",
      "0.3341278309999325\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (549, 54) (612, 145)\n",
      "0.31470110900045256\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (548, 54) (612, 146)\n",
      "0.3073549809996621\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (548, 54) (612, 146)\n",
      "0.3158456749988545\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (548, 54) (611, 146)\n",
      "0.3288425669998105\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (548, 53) (611, 146)\n",
      "0.30775669799913885\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (547, 54) (611, 147)\n",
      "0.31578542800161813\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (547, 54) (611, 147)\n",
      "0.32556307400045625\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (546, 54) (610, 145)\n",
      "0.30994135800028744\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (546, 53) (610, 145)\n",
      "0.3079407490004087\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (545, 54) (610, 145)\n",
      "0.31930306799949904\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (545, 54) (610, 147)\n",
      "0.32151509600043937\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (545, 54) (610, 147)\n",
      "0.31707515499874717\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (545, 55) (610, 148)\n",
      "0.31892362299913657\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 55) (610, 148)\n",
      "0.31513433799955237\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 55) (610, 148)\n",
      "0.31729612100025406\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 55) (610, 147)\n",
      "0.3081643079985952\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (610, 147)\n",
      "0.31740374800028803\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (609, 147)\n",
      "0.31296298099914566\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (609, 147)\n",
      "0.3148989109995455\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 53) (609, 148)\n",
      "0.32866477799871063\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 54) (609, 148)\n",
      "0.32387551400097436\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 53) (610, 148)\n",
      "0.31240719800007355\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 53) (609, 147)\n",
      "0.31202211100026034\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 53) (609, 147)\n",
      "0.3078404910011159\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 52) (609, 146)\n",
      "0.3165366199991695\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 53) (609, 148)\n",
      "0.33178692100045737\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (610, 150)\n",
      "0.3124334349995479\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (610, 150)\n",
      "0.3166685550004331\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (610, 149)\n",
      "0.3131294369995885\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (610, 148)\n",
      "0.31237546800002747\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (610, 147)\n",
      "0.32826873699923453\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (610, 146)\n",
      "0.3111105169991788\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (611, 147)\n",
      "0.3094323000004806\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (611, 149)\n",
      "0.31057811900063825\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (611, 149)\n",
      "0.3136335990002408\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (545, 54) (611, 150)\n",
      "0.3193708059989149\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (545, 54) (611, 150)\n",
      "0.3644202010000299\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (611, 151)\n",
      "0.3225281939994602\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (611, 150)\n",
      "0.3141034890013543\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 55) (611, 150)\n",
      "0.31999901500057604\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 54) (611, 149)\n",
      "0.3198328449998371\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (611, 147)\n",
      "0.31266614900050627\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (611, 147)\n",
      "0.31728815700080304\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (610, 147)\n",
      "0.30885480499819096\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 53) (610, 147)\n",
      "0.3114717340013158\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 53) (610, 147)\n",
      "0.329020897999726\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 53) (609, 147)\n",
      "0.3154055370014248\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 53) (609, 147)\n",
      "0.3374348399993323\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 52) (609, 147)\n",
      "0.36964977299976454\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 52) (610, 146)\n",
      "0.34059117600008904\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 51) (610, 146)\n",
      "0.32570079200013424\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 51) (609, 143)\n",
      "0.31558101600057853\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (542, 50) (609, 143)\n",
      "0.3093131790010375\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (542, 50) (609, 142)\n",
      "0.3027017100012017\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (542, 49) (610, 141)\n",
      "0.3069880559996818\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (542, 49) (610, 141)\n",
      "0.3040171110005758\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (542, 50) (610, 141)\n",
      "0.30339395300143224\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 50) (610, 142)\n",
      "0.2990096760004235\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 50) (609, 143)\n",
      "0.2993001020004158\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 50) (609, 143)\n",
      "0.3042701850008598\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 50) (609, 143)\n",
      "0.30892384899925673\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 50) (610, 142)\n",
      "0.29765220300032524\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 50) (610, 142)\n",
      "0.3071571960008441\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 50) (610, 142)\n",
      "0.2965218030003598\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 51) (609, 144)\n",
      "0.30444964100024663\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 52) (609, 146)\n",
      "0.3283697120004945\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 54) (609, 148)\n",
      "0.3008842379986163\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 55) (610, 149)\n",
      "0.3152055049995397\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 57) (610, 149)\n",
      "0.2988418249988172\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 58) (610, 151)\n",
      "0.3010271580005792\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 58) (611, 151)\n",
      "0.3372474030002195\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 59) (610, 151)\n",
      "0.33188913599951775\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (544, 58) (611, 153)\n",
      "0.30687817800026096\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (541, 64) (611, 160)\n",
      "0.30694729700007883\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (541, 64) (611, 160)\n",
      "0.3016297409994877\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (541, 65) (611, 160)\n",
      "0.31232118899970374\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (439, 51) (542, 186)\n",
      "0.3063379380000697\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (443, 53) (548, 185)\n",
      "0.3200811490005435\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (446, 42) (550, 193)\n",
      "0.3041385390006326\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (444, 54) (550, 183)\n",
      "0.3135705089989642\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (449, 43) (550, 193)\n",
      "0.33226765600011277\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (449, 43) (551, 194)\n",
      "0.33106192000013834\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (451, 44) (552, 196)\n",
      "0.3326563709997572\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (451, 45) (553, 198)\n",
      "0.3416439810007432\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (452, 46) (554, 199)\n",
      "0.32978067499971075\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (454, 49) (556, 201)\n",
      "0.32358239100176434\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (456, 50) (557, 202)\n",
      "0.32215151399941533\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (458, 51) (559, 203)\n",
      "0.34164601800148375\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (459, 52) (560, 202)\n",
      "0.3353009459988243\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (461, 53) (560, 200)\n",
      "0.391284650999296\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (462, 53) (560, 199)\n",
      "0.31312935200003267\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (464, 54) (561, 199)\n",
      "0.3288821820005978\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (466, 56) (561, 198)\n",
      "0.34526509099850955\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (469, 56) (564, 197)\n",
      "0.31555366199972923\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (471, 57) (565, 197)\n",
      "0.3279564419990493\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (473, 59) (567, 197)\n",
      "0.3262663910008996\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (475, 59) (567, 199)\n",
      "0.3275374179993378\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (477, 60) (568, 199)\n",
      "0.3236340840012417\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (479, 58) (577, 198)\n",
      "0.32358344699969166\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (481, 59) (578, 199)\n",
      "0.3397113099999842\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (482, 56) (579, 199)\n",
      "0.3327840770016337\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (484, 56) (580, 200)\n",
      "0.38159100400116586\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (485, 55) (581, 200)\n",
      "0.3540238440000394\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (487, 55) (583, 200)\n",
      "0.35163578600077017\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (488, 55) (585, 199)\n",
      "0.3624241979996441\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (490, 55) (587, 199)\n",
      "0.3553408789994137\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (492, 53) (590, 200)\n",
      "0.37040406099913525\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (493, 53) (592, 201)\n",
      "0.36185947400008445\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (494, 52) (593, 199)\n",
      "0.35074456999973336\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (496, 52) (593, 198)\n",
      "0.34479269799885515\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (497, 52) (595, 196)\n",
      "0.3564150130005146\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (499, 52) (596, 194)\n",
      "0.3414900200004922\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (502, 53) (598, 194)\n",
      "0.3193455119999271\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (504, 51) (600, 188)\n",
      "0.3521350219998567\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (504, 48) (600, 186)\n",
      "0.35629559900007735\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (506, 48) (600, 180)\n",
      "0.34658693999881507\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (508, 48) (602, 181)\n",
      "0.34886889700101165\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (513, 46) (608, 185)\n",
      "0.36845913599972846\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (512, 46) (610, 186)\n",
      "0.3339130579988705\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (513, 47) (611, 185)\n",
      "0.3217765169993072\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (514, 47) (611, 185)\n",
      "0.31851100499989116\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (515, 46) (612, 185)\n",
      "0.31664204300068377\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (516, 47) (613, 185)\n",
      "0.3518366150001384\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (518, 48) (613, 184)\n",
      "0.37779479600067134\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (520, 48) (614, 184)\n",
      "0.35498353900038637\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (522, 48) (615, 183)\n",
      "0.3797826339996391\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (524, 48) (616, 183)\n",
      "0.3223768019997806\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (526, 48) (618, 185)\n",
      "0.350848361998942\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (527, 48) (619, 186)\n",
      "0.34576673500123434\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (529, 49) (620, 187)\n",
      "0.3755033200013713\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (532, 50) (624, 186)\n",
      "0.34764383400033694\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (536, 52) (628, 185)\n",
      "0.37350770400007605\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (539, 53) (631, 184)\n",
      "0.34666719600136275\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (541, 54) (633, 185)\n",
      "0.34636829599912744\n",
      "(416, 416, 3)\n",
      "Found 1 boxes for img\n",
      "face 1.00 (543, 56) (634, 184)\n",
      "0.3710057260013855\n",
      "(416, 416, 3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-06226b7405d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdetect_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mface_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sample_video1.mp4\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-f293dfeceb3e>\u001b[0m in \u001b[0;36mdetect_video\u001b[0;34m(yolo, video_path, output_path)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myolo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mcurr_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-a951f3288549>\u001b[0m in \u001b[0;36mdetect_image\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mimage_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Add batch dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         out_boxes, out_scores, out_classes = self.sess.run(\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             feed_dict={\n",
      "\u001b[0;32m/media/hoangle/Works/anaconda/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/hoangle/Works/anaconda/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/hoangle/Works/anaconda/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1369\u001b[0m                            run_metadata)\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/hoangle/Works/anaconda/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/hoangle/Works/anaconda/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/hoangle/Works/anaconda/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1450\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1451\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m                                             run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "detect_video(YOLO(**face_config),\"sample_video1.mp4\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
